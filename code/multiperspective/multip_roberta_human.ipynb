{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification multi-perspective approach RoBERTa-large HHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['soft_labels'] = train['soft_labels'].apply(ast.literal_eval).tolist()\n",
    "test['soft_labels'] = test['soft_labels'].apply(ast.literal_eval).tolist()\n",
    "val['soft_labels'] = val['soft_labels'].apply(ast.literal_eval).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "val = Dataset.from_pandas(val)\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict() \n",
    "dataset['train'] = train\n",
    "dataset['val'] = val\n",
    "dataset['test'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-large'\n",
    "model_name_filename = model_name.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    # Tokenize the input text and map the 'soft_labels' column to 'labels'\n",
    "    tokenized_inputs = tokenizer(examples['Input'], padding='max_length', truncation=True, max_length=512)\n",
    "    tokenized_inputs['labels'] = examples['soft_labels'] \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train.map(tokenize_func, batched = True)\n",
    "val_tokenized = val.map(tokenize_func, batched = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized.set_format('torch', columns =['input_ids', 'attention_mask', 'labels'])\n",
    "val_tokenized.set_format('torch', columns = ['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=len(train_tokenized['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./multiclassification/{model_name_filename}/results/human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition soft loss function\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities from logits.\n",
    "    \n",
    "    Parameters:\n",
    "    - logits: A numpy array of shape (n, num_classes) containing the logits.\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: A numpy array of shape (n, num_classes) containing the softmax probabilities.\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def soft_loss_function(true_probabilities, predicted_logits):\n",
    "    \"\"\"\n",
    "    Compute the soft loss function (cross-entropy with soft labels) using PyTorch tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    - true_probabilities: A PyTorch tensor of shape (n, num_classes) containing the true probability distributions.\n",
    "    - predicted_logits: A PyTorch tensor of shape (n, num_classes) containing the logits from the model.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: The computed soft loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_probabilities = torch.nn.functional.softmax(predicted_logits, dim=-1)\n",
    "    \n",
    "   \n",
    "    epsilon = 1e-15\n",
    "    predicted_probabilities = torch.clamp(predicted_probabilities, epsilon, 1. - epsilon)\n",
    "    \n",
    "   \n",
    "    loss = -torch.sum(true_probabilities * torch.log(predicted_probabilities))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Compute the loss for the model. This uses the soft_loss_function.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The model to evaluate.\n",
    "        - inputs: A dictionary of inputs to the model.\n",
    "        - return_outputs: Whether to return model outputs along with loss.\n",
    "\n",
    "        Returns:\n",
    "        - loss: The computed loss.\n",
    "        - outputs (optional): The model outputs, if return_outputs is True.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "      \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "       \n",
    "        true_probabilities = labels \n",
    "        logits = logits \n",
    "        \n",
    "        \n",
    "        loss = soft_loss_function(true_probabilities, logits)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = test.map(tokenize_func, batched=True)\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts, model, tokenizer, device):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "batch_size = 8\n",
    "\n",
    "for i in range(0, len(test), batch_size):\n",
    "    batch = test[i:i+batch_size]\n",
    "    batch_texts = batch['Input']\n",
    "    batch_predictions = predict(batch_texts, model, tokenizer, device)\n",
    "    all_predictions.extend(batch_predictions)\n",
    "    all_labels.extend(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "# Apply softmax to predictions\n",
    "softmax_predictions = softmax(all_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Predicted_scores'] = all_predictions.tolist()\n",
    "df_test['Predicted_Softmax_scores'] = softmax_predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_labels'] = df_test['Predicted_scores'].apply(lambda x: x.index(max(x)))\n",
    "df_test['predicted_softmax_labels'] = df_test['Predicted_Softmax_scores'].apply(lambda x: x.index(max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "test = Dataset.from_pandas(df_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "label_ecnoding_multip = {'Pro': 0,\n",
    "'Against': 1,\n",
    "'Neutral': 2,\n",
    "'Not-about': 3,}\n",
    "\n",
    "\n",
    "m_test['true_labels'] = m_test['majority_label'].map(label_ecnoding_multip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate classification metrics \n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def calculate_confidences(df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate confidence scores and update the DataFrame with a new column.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame with input data\n",
    "    - model: Trained model with a method to get logits\n",
    "    - tokenizer: Tokenizer to preprocess text\n",
    "    \n",
    "    Returns:\n",
    "    - df: Updated DataFrame with a 'confidence_scores' column\n",
    "    \"\"\"\n",
    "    confidences = []\n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Input']  \n",
    "        \n",
    "       \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probabilities = F.softmax(logits, dim=-1)  \n",
    "            probabilities = probabilities.cpu().numpy().flatten()  \n",
    "            \n",
    "           \n",
    "            max_prob = np.max(probabilities)\n",
    "            confidences.append(max_prob)\n",
    "    \n",
    "   \n",
    "    df['confidence_scores'] = confidences\n",
    "    return df\n",
    "\n",
    "\n",
    "m_test = calculate_confidences(m_test, model, tokenizer)\n",
    "\n",
    "# Proceed with metrics calculation\n",
    "y_true = m_test['true_labels']\n",
    "y_pred = m_test['predicted_labels']\n",
    "confidence_scores = m_test['confidence_scores']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision * 100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall * 100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1 * 100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Average Confidence Score\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "print(\"Average Confidence Score:\", avg_confidence * 100)\n",
    "\n",
    "# Confidence for Correct and Incorrect Predictions\n",
    "correct_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred == true])\n",
    "incorrect_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred != true])\n",
    "\n",
    "print(\"Average Confidence for Correct Predictions:\", correct_confidence * 100)\n",
    "print(\"Average Confidence for Incorrect Predictions:\", incorrect_confidence * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft loss function definition and temperature module\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities from logits.\n",
    "    \n",
    "    Parameters:\n",
    "    - logits: A numpy array of shape (n, num_classes) containing the logits.\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: A numpy array of shape (n, num_classes) containing the softmax probabilities.\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def soft_loss_function(true_probabilities, predicted_logits):\n",
    "    \"\"\"\n",
    "    Compute the soft loss function (cross-entropy with soft labels) using PyTorch tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    - true_probabilities: A PyTorch tensor of shape (n, num_classes) containing the true probability distributions.\n",
    "    - predicted_logits: A PyTorch tensor of shape (n, num_classes) containing the logits from the model.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: The computed soft loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_probabilities = torch.nn.functional.softmax(predicted_logits, dim=-1)\n",
    "    \n",
    "\n",
    "    epsilon = 1e-15\n",
    "    predicted_probabilities = torch.clamp(predicted_probabilities, epsilon, 1. - epsilon)\n",
    "    \n",
    "  \n",
    "    loss = -torch.sum(true_probabilities * torch.log(predicted_probabilities))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class TemperatureScalingCalibration(nn.Module):\n",
    "    def __init__(self, model_path: str, tokenizer, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.model_path = model_path\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "        self.temperature = nn.Parameter(torch.ones(1)) \n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.temperature.to(self.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward method that returns softmax-ed confidence scores.\"\"\"\n",
    "        logits = self.forward_logit(input_ids, attention_mask)\n",
    "        scaled_logits = logits / self.temperature\n",
    "        scores = nn.functional.softmax(scaled_logits, dim=-1)\n",
    "        return scores\n",
    "\n",
    "    def forward_logit(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward method that returns logits, to be used with cross-entropy loss.\"\"\"\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "        return outputs\n",
    "\n",
    "    def fit(self, dataset_tokenized, n_epochs: int = 3, batch_size: int = 64, lr: float = 0.01):\n",
    "        \"\"\"Fits the temperature scaling parameter.\"\"\"\n",
    "        data_collator = DataCollatorWithPadding(self.tokenizer, padding=True)\n",
    "        data_loader = DataLoader(dataset_tokenized, collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "        self.freeze_base_model()\n",
    "      \n",
    "  \n",
    "        optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "\n",
    "      \n",
    "        self.train()\n",
    "\n",
    "        for epoch in trange(n_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for examples in data_loader:\n",
    "                \n",
    "                input_ids = examples['input_ids'].to(self.device)\n",
    "                attention_mask = examples['attention_mask'].to(self.device)\n",
    "                soft_labels = examples['labels'].to(self.device)  \n",
    "                \n",
    "                \n",
    "                self.zero_grad()\n",
    "                logits = self.forward_logit(input_ids, attention_mask)\n",
    "                \n",
    "                \n",
    "                scaled_logits = logits / self.temperature\n",
    "                \n",
    "                \n",
    "                loss = soft_loss_function(soft_labels, scaled_logits)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "                epoch_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "        \n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss / len(dataset_tokenized)}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def freeze_base_model(self):\n",
    "        \"\"\"Remember to freeze base model's parameters when training temperature scaler.\"\"\"\n",
    "        self.model.eval()\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "calibration_module = TemperatureScalingCalibration(model_path=output_dir, tokenizer=tokenizer, device=device)\n",
    "calibration_module.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    # Tokenize the input text and map the 'soft_labels' column to 'labels'\n",
    "    tokenized_inputs = tokenizer(examples['Input'], padding='max_length', truncation=True, max_length=512)\n",
    "    tokenized_inputs['labels'] = examples['soft_labels'] \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenized_cal = val.map(tokenize_func, batched = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenized_cal.set_format('torch', columns = ['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_module.fit(val_tokenized_cal,n_epochs=6, batch_size=64, lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_module.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cal(texts, model, tokenizer, device):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']} \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)  \n",
    "    return logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "batch_size = 8  \n",
    "\n",
    "for i in range(0, len(test), batch_size):\n",
    "    batch = test[i:i+batch_size]\n",
    "    batch_texts = batch['Input']\n",
    "    batch_predictions = predict_cal(batch_texts, calibration_module, tokenizer, device)\n",
    "    all_predictions.extend(batch_predictions)\n",
    "    all_labels.extend(batch['labels']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "# Apply softmax to predictions\n",
    "softmax_predictions = softmax(all_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cal = test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cal['Predicted_scores'] = all_predictions.tolist()\n",
    "df_test_cal['Predicted_Softmax_scores'] = softmax_predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cal['predicted_labels'] = df_test['Predicted_scores'].apply(lambda x: x.index(max(x)))\n",
    "df_test_cal['predicted_softmax_labels'] = df_test['Predicted_Softmax_scores'].apply(lambda x: x.index(max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "test_cal = Dataset.from_pandas(df_test_cal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ecnoding_multip = {'Pro': 0,\n",
    "'Against': 1,\n",
    "'Neutral': 2,\n",
    "'Not-about': 3,} \n",
    "\n",
    "\n",
    "m_test_cal['true_labels'] = m_test_cal['majority_label'].map(label_ecnoding_multip)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def calculate_confidences_cal(df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate confidence scores and update the DataFrame with a new column.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame with input data\n",
    "    - model: Trained model with a method to get logits\n",
    "    - tokenizer: Tokenizer to preprocess text\n",
    "    \n",
    "    Returns:\n",
    "    - df: Updated DataFrame with a 'confidence_scores' column\n",
    "    \"\"\"\n",
    "    confidences = []\n",
    "\n",
    "    model.eval()  \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Input']  \n",
    "        \n",
    "     \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probabilities = F.softmax(logits, dim=-1)  \n",
    "            probabilities = probabilities.cpu().numpy().flatten()  \n",
    "            \n",
    "           \n",
    "            max_prob = np.max(probabilities)\n",
    "            confidences.append(max_prob)\n",
    "    \n",
    "  \n",
    "    df['confidence_scores'] = confidences\n",
    "    return df\n",
    "\n",
    "\n",
    "m_test_cal = calculate_confidences_cal(m_test_cal, model, tokenizer)\n",
    "\n",
    "\n",
    "y_true = m_test_cal['true_labels']\n",
    "y_pred = m_test_cal['predicted_labels']\n",
    "confidence_scores = m_test_cal['confidence_scores']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision * 100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall * 100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1 * 100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Average Confidence Score\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "print(\"Average Confidence Score:\", avg_confidence * 100)\n",
    "\n",
    "# Confidence for Correct and Incorrect Predictions\n",
    "correct_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred == true])\n",
    "incorrect_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred != true])\n",
    "\n",
    "print(\"Average Confidence for Correct Predictions:\", correct_confidence * 100)\n",
    "print(\"Average Confidence for Incorrect Predictions:\", incorrect_confidence * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multip",
   "language": "python",
   "name": "multip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
