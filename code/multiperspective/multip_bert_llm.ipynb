{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification multi-perspective approach with BERT-large LLMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to delete no majority instances \n",
    "def clean_data(df, col):\n",
    "     df = df.loc[~(df[col] == 'No Majority')] \n",
    "     return df\n",
    "\n",
    "\n",
    " train = clean_data(train_df, 'majority_llm_noninst')\n",
    " test = clean_data(test_df, 'majority_llm_noninst')\n",
    " val = clean_data(val_df, 'majority_llm_noninst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping labels\n",
    "labels = ['Pro', 'Against', 'Neutral', 'Not-about']\n",
    "num_labels = len(labels)\n",
    "id2label = {id:label for id,label in enumerate(labels)}\n",
    "label2id = {label:id for id,label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "val = Dataset.from_pandas(val)\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict() \n",
    "dataset['train'] = train\n",
    "dataset['val'] = val\n",
    "dataset['test'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "model_name = 'bert-large-uncased'\n",
    "model_name_filename = model_name.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing soft labels\n",
    "\n",
    "\n",
    "def parse_soft_labels(example):\n",
    "    example['soft_labels_noninst'] = ast.literal_eval(example['soft_labels_noninst'])\n",
    "    return example\n",
    "\n",
    "\n",
    "train = train.map(parse_soft_labels)\n",
    "test = test.map(parse_soft_labels)\n",
    "val = val.map(parse_soft_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    # Tokenize the input text and map the 'soft_labels' column to 'labels'\n",
    "    tokenized_inputs = tokenizer(examples['Input'], padding='max_length', truncation=True, max_length=512)\n",
    "    tokenized_inputs['labels'] = examples['soft_labels_noninst']  # Rename 'soft_labels' to 'labels'\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train.map(tokenize_func, batched = True)\n",
    "val_tokenized = val.map(tokenize_func, batched = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized.set_format('torch', columns =['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "val_tokenized.set_format('torch', columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=len(train_tokenized['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./multiclassification/{model_name_filename}/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of soft_loss_function \n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities from logits.\n",
    "    \n",
    "    Parameters:\n",
    "    - logits: A numpy array of shape (n, num_classes) containing the logits.\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: A numpy array of shape (n, num_classes) containing the softmax probabilities.\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def soft_loss_function(true_probabilities, predicted_logits):\n",
    "    \"\"\"\n",
    "    Compute the soft loss function (cross-entropy with soft labels) using PyTorch tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    - true_probabilities: A PyTorch tensor of shape (n, num_classes) containing the true probability distributions.\n",
    "    - predicted_logits: A PyTorch tensor of shape (n, num_classes) containing the logits from the model.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: The computed soft loss.\n",
    "    \"\"\"\n",
    "  \n",
    "    predicted_probabilities = torch.nn.functional.softmax(predicted_logits, dim=-1)\n",
    "    \n",
    "   \n",
    "    epsilon = 1e-15\n",
    "    predicted_probabilities = torch.clamp(predicted_probabilities, epsilon, 1. - epsilon)\n",
    "    \n",
    "  \n",
    "    loss = -torch.sum(true_probabilities * torch.log(predicted_probabilities))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Compute the loss for the model. This uses the soft_loss_function.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The model to evaluate.\n",
    "        - inputs: A dictionary of inputs to the model.\n",
    "        - return_outputs: Whether to return model outputs along with loss.\n",
    "\n",
    "        Returns:\n",
    "        - loss: The computed loss.\n",
    "        - outputs (optional): The model outputs, if return_outputs is True.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "      \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "      \n",
    "        true_probabilities = labels  \n",
    "        logits = logits  \n",
    "        \n",
    "      \n",
    "        loss = soft_loss_function(true_probabilities, logits)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = test.map(tokenize_func, batched=True)\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", 'token_type_ids', \"labels\"])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction phase\n",
    "def predict(texts, model, tokenizer, device):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "batch_size = 8  \n",
    "\n",
    "for i in range(0, len(test), batch_size):\n",
    "    batch = test[i:i+batch_size]\n",
    "    batch_texts = batch['Input']\n",
    "    batch_predictions = predict(batch_texts, model, tokenizer, device)\n",
    "    all_predictions.extend(batch_predictions)\n",
    "    all_labels.extend(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "# Apply softmax to predictions\n",
    "softmax_predictions = softmax(all_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting predicted scores and normalized predicted scores with softmax\n",
    "df_test['Predicted_scores'] = all_predictions.tolist()\n",
    "df_test['Predicted_Softmax_scores'] = softmax_predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_labels'] = df_test['Predicted_scores'].apply(lambda x: x.index(max(x)))\n",
    "df_test['predicted_softmax_labels'] = df_test['Predicted_Softmax_scores'].apply(lambda x: x.index(max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading test file\n",
    "from datasets import Dataset\n",
    "test = Dataset.from_pandas(df_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test file but changed name \n",
    "m_test[\"majority_llm_noninst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "label_ecnoding_multip = {'Pro': 0,\n",
    "'Against': 1,\n",
    "'Neutral': 2,\n",
    "'Not-about': 3,}\n",
    "\n",
    "\n",
    "m_test['true_labels'] = m_test['majority_llm_noninst'].map(label_ecnoding_multip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation confidence and classification metrics (accuracy, precision, recall, f1, confusion matrix)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def calculate_confidences(df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate confidence scores and update the DataFrame with a new column.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame with input data\n",
    "    - model: Trained model with a method to get logits\n",
    "    - tokenizer: Tokenizer to preprocess text\n",
    "    \n",
    "    Returns:\n",
    "    - df: Updated DataFrame with a 'confidence_scores' column\n",
    "    \"\"\"\n",
    "    confidences = []\n",
    "\n",
    "    model.eval()  \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Input']  \n",
    "        \n",
    "       \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probabilities = F.softmax(logits, dim=-1)  \n",
    "            probabilities = probabilities.cpu().numpy().flatten()  \n",
    "            \n",
    "            \n",
    "            max_prob = np.max(probabilities)\n",
    "            confidences.append(max_prob)\n",
    "    \n",
    "   \n",
    "    df['confidence_scores'] = confidences\n",
    "    return df\n",
    "\n",
    "\n",
    "m_test = calculate_confidences(m_test, model, tokenizer)\n",
    "\n",
    "\n",
    "y_true = m_test['true_labels']\n",
    "y_pred = m_test['predicted_labels']\n",
    "confidence_scores = m_test['confidence_scores']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision * 100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall * 100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1 * 100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Average Confidence Score\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "print(\"Average Confidence Score:\", avg_confidence * 100)\n",
    "\n",
    "# Confidence for Correct and Incorrect Predictions\n",
    "correct_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred == true])\n",
    "incorrect_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred != true])\n",
    "\n",
    "print(\"Average Confidence for Correct Predictions:\", correct_confidence * 100)\n",
    "print(\"Average Confidence for Incorrect Predictions:\", incorrect_confidence * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 61.76470588235294\n",
    "Precision: 15.441176470588236\n",
    "Recall: 25.0\n",
    "F1 Score: 19.090909090909093\n",
    "Confusion Matrix:\n",
    "[[63  0  0  0]\n",
    " [13  0  0  0]\n",
    " [ 6  0  0  0]\n",
    " [20  0  0  0]]\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.62      1.00      0.76        63\n",
    "           1       0.00      0.00      0.00        13\n",
    "           2       0.00      0.00      0.00         6\n",
    "           3       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.62       102\n",
    "   macro avg       0.15      0.25      0.19       102\n",
    "weighted avg       0.38      0.62      0.47       102\n",
    "\n",
    "Average Confidence Score: 45.91071307659149\n",
    "Average Confidence for Correct Predictions: 46.44718411422911\n",
    "Average Confidence for Incorrect Predictions: 45.044117325391525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft loss function definition and application on temperature scaling\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities from logits.\n",
    "    \n",
    "    Parameters:\n",
    "    - logits: A numpy array of shape (n, num_classes) containing the logits.\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: A numpy array of shape (n, num_classes) containing the softmax probabilities.\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def soft_loss_function(true_probabilities, predicted_logits):\n",
    "    \"\"\"\n",
    "    Compute the soft loss function (cross-entropy with soft labels) using PyTorch tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    - true_probabilities: A PyTorch tensor of shape (n, num_classes) containing the true probability distributions.\n",
    "    - predicted_logits: A PyTorch tensor of shape (n, num_classes) containing the logits from the model.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: The computed soft loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_probabilities = torch.nn.functional.softmax(predicted_logits, dim=-1)\n",
    "    \n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    predicted_probabilities = torch.clamp(predicted_probabilities, epsilon, 1. - epsilon)\n",
    "    \n",
    "  \n",
    "    loss = -torch.sum(true_probabilities * torch.log(predicted_probabilities))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class TemperatureScalingCalibration(nn.Module):\n",
    "    def __init__(self, model_path: str, tokenizer, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.model_path = model_path\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "     \n",
    "        self.temperature = nn.Parameter(torch.ones(1)) \n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.temperature.to(self.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward method that returns softmax-ed confidence scores.\"\"\"\n",
    "        logits = self.forward_logit(input_ids, attention_mask)\n",
    "        scaled_logits = logits / self.temperature\n",
    "        scores = nn.functional.softmax(scaled_logits, dim=-1)\n",
    "        return scores\n",
    "\n",
    "    def forward_logit(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward method that returns logits, to be used with cross-entropy loss.\"\"\"\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "        return outputs\n",
    "\n",
    "    def fit(self, dataset_tokenized, n_epochs: int = 3, batch_size: int = 64, lr: float = 0.01):\n",
    "        \"\"\"Fits the temperature scaling parameter.\"\"\"\n",
    "        data_collator = DataCollatorWithPadding(self.tokenizer, padding=True)\n",
    "        data_loader = DataLoader(dataset_tokenized, collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "        self.freeze_base_model()\n",
    "      \n",
    "       \n",
    "        optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "\n",
    "      \n",
    "        self.train()\n",
    "\n",
    "        for epoch in trange(n_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for examples in data_loader:\n",
    "           \n",
    "                input_ids = examples['input_ids'].to(self.device)\n",
    "                attention_mask = examples['attention_mask'].to(self.device)\n",
    "                soft_labels = examples['labels'].to(self.device)  \n",
    "                \n",
    "              \n",
    "                self.zero_grad()\n",
    "                logits = self.forward_logit(input_ids, attention_mask)\n",
    "                \n",
    "               \n",
    "                scaled_logits = logits / self.temperature\n",
    "                \n",
    "                \n",
    "                loss = soft_loss_function(soft_labels, scaled_logits)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "               \n",
    "                epoch_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "           \n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss / len(dataset_tokenized)}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def freeze_base_model(self):\n",
    "        \"\"\"Remember to freeze base model's parameters when training temperature scaler.\"\"\"\n",
    "        self.model.eval()\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving calibration module\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "calibration_module = TemperatureScalingCalibration(model_path=output_dir, tokenizer=tokenizer, device=device)\n",
    "calibration_module.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    # Tokenize the input text and map the 'soft_labels' column to 'labels'\n",
    "    tokenized_inputs = tokenizer(examples['Input'], padding='max_length', truncation=True, max_length=512)\n",
    "    tokenized_inputs['labels'] = examples['soft_labels_noninst']  # Rename 'soft_labels' to 'labels'\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizaton of evaluation data\n",
    "val_tokenized_cal = val.map(tokenize_func, batched = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenized_cal.set_format('torch', columns = ['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting of the calibrated module \n",
    "calibration_module.fit(val_tokenized_cal,n_epochs=6, batch_size=64, lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cal(texts, model, tokenizer, device):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', \"token_ids\", 'attention_mask']}  \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)  \n",
    "    return logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "batch_size = 8  \n",
    "\n",
    "for i in range(0, len(test), batch_size):\n",
    "    batch = test[i:i+batch_size]\n",
    "    batch_texts = batch['Input']\n",
    "    batch_predictions = predict_cal(batch_texts, calibration_module, tokenizer, device)\n",
    "    all_predictions.extend(batch_predictions)\n",
    "    all_labels.extend(batch['labels']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "# Apply softmax to predictions\n",
    "softmax_predictions = softmax(all_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cal = test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cal['Predicted_scores'] = all_predictions.tolist()\n",
    "df_test_cal['Predicted_Softmax_scores'] = softmax_predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cal['predicted_labels'] = df_test['Predicted_scores'].apply(lambda x: x.index(max(x)))\n",
    "df_test_cal['predicted_softmax_labels'] = df_test['Predicted_Softmax_scores'].apply(lambda x: x.index(max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "test_cal = Dataset.from_pandas(df_test_cal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ecnoding_multip = {'Pro': 0,\n",
    "'Against': 1,\n",
    "'Neutral': 2,\n",
    "'Not-about': 3,} \n",
    "\n",
    "\n",
    "m_test_cal['true_labels'] = m_test_cal['majority_llm_noninst'].map(label_ecnoding_multip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of confidence and classification metrics (accuracy, precision, recall, f1, confusion matrix) for calibrated module\n",
    "\n",
    "def calculate_confidences_cal(df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate confidence scores and update the DataFrame with a new column.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame with input data\n",
    "    - model: Trained model with a method to get logits\n",
    "    - tokenizer: Tokenizer to preprocess text\n",
    "    \n",
    "    Returns:\n",
    "    - df: Updated DataFrame with a 'confidence_scores' column\n",
    "    \"\"\"\n",
    "    confidences = []\n",
    "\n",
    "    calibration_module.eval()  \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Input']  \n",
    "        \n",
    "   \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probabilities = F.softmax(logits, dim=-1)  \n",
    "            probabilities = probabilities.cpu().numpy().flatten()  \n",
    "            \n",
    "            \n",
    "            max_prob = np.max(probabilities)\n",
    "            confidences.append(max_prob)\n",
    "    \n",
    "   \n",
    "    df['confidence_scores'] = confidences\n",
    "    return df\n",
    "\n",
    "\n",
    "m_test_cal = calculate_confidences_cal(m_test_cal, model, tokenizer)\n",
    "\n",
    "\n",
    "y_true = m_test_cal['true_labels']\n",
    "y_pred = m_test_cal['predicted_labels']\n",
    "confidence_scores = m_test_cal['confidence_scores']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision * 100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall * 100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1 * 100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Average Confidence Score\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "print(\"Average Confidence Score:\", avg_confidence * 100)\n",
    "\n",
    "# Confidence for Correct and Incorrect Predictions\n",
    "correct_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred == true])\n",
    "incorrect_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred != true])\n",
    "\n",
    "print(\"Average Confidence for Correct Predictions:\", correct_confidence * 100)\n",
    "print(\"Average Confidence for Incorrect Predictions:\", incorrect_confidence * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 61.76470588235294\n",
    "Precision: 15.441176470588236\n",
    "Recall: 25.0\n",
    "F1 Score: 19.090909090909093\n",
    "Confusion Matrix:\n",
    "[[63  0  0  0]\n",
    " [13  0  0  0]\n",
    " [ 6  0  0  0]\n",
    " [20  0  0  0]]\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.62      1.00      0.76        63\n",
    "           1       0.00      0.00      0.00        13\n",
    "           2       0.00      0.00      0.00         6\n",
    "           3       0.00      0.00      0.00        20\n",
    "\n",
    "    accuracy                           0.62       102\n",
    "   macro avg       0.15      0.25      0.19       102\n",
    "weighted avg       0.38      0.62      0.47       102\n",
    "\n",
    "Average Confidence Score: 45.91071307659149\n",
    "Average Confidence for Correct Predictions: 46.44718411422911\n",
    "Average Confidence for Incorrect Predictions: 45.044117325391525"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multip",
   "language": "python",
   "name": "multip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
