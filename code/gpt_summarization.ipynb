{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja_MxtVvFT8f",
        "outputId": "8a027c58-e4e9-4e87-fd14-4f7f630da770"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dnwDTHLFum2"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import os\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cCxe9acF48O"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Perspectivism/Dataset/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Perspectivism/Dataset/test.csv')\n",
        "val = pd.read_csv('/content/drive/MyDrive/Perspectivism/Dataset/val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVKFiPx9F8UV",
        "outputId": "6f06aec3-0c3d-4df1-f923-c5a12c358688"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([train, test, val], axis=0)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwGfwWDCF8W9"
      },
      "outputs": [],
      "source": [
        "b_1 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_1.csv')\n",
        "b_2 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_2.csv')\n",
        "b_3 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_3.csv')\n",
        "b_4 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_4.csv')\n",
        "b_5 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_5.csv')\n",
        "b_6 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_6.csv')\n",
        "b_7 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_7.csv')\n",
        "b_8 = pd.read_csv('/content/drive/MyDrive/Perspectivism/GPT/old_batches/batch_8.csv')\n",
        "\n",
        "all_batches = pd.concat([b_1, b_2, b_3, b_4, b_5, b_6, b_7, b_8]) # read from batches\n",
        "all_batches.rename(columns={'custom_id': 'docID', 'content': 'gpt_summaries'}, inplace=True)\n",
        "\n",
        "# Merge with the main DataFrame\n",
        "df = df.merge(all_batches, how='left', on='docID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpuEqbJsGQBc",
        "outputId": "ca31f3f7-946e-44e7-9c7e-ef337094a6f3"
      },
      "outputs": [],
      "source": [
        "df = df[df['gpt_summaries'].isna()]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAHcHgWKG3DF"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key= \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chWIDEw-JLTx"
      },
      "outputs": [],
      "source": [
        "def process_response_to_dataframe(response_text):\n",
        "    responses = response_text.strip().split(\"\\n\")\n",
        "    data = []\n",
        "    for response in responses:\n",
        "        json_response = json.loads(response)\n",
        "        custom_id = json_response.get('custom_id')\n",
        "        content = json_response.get('response', {}).get('body', {}).get('choices', [{}])[0].get('message', {}).get('content')\n",
        "        data.append({\"custom_id\": custom_id, \"content\": content})\n",
        "    batch = pd.DataFrame(data)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTIDz9oTJSeq"
      },
      "outputs": [],
      "source": [
        "def submit_batch(file_path, description):\n",
        "    batch_input_file = client.files.create(\n",
        "        file=open(file_path, \"rb\"),\n",
        "        purpose=\"batch\"\n",
        "    )\n",
        "    batch_input_file_id = batch_input_file.id\n",
        "\n",
        "    batch = client.batches.create(\n",
        "        input_file_id=batch_input_file_id,\n",
        "        endpoint=\"/v1/chat/completions\",\n",
        "        completion_window=\"24h\",\n",
        "        metadata={\n",
        "            \"description\": description\n",
        "        }\n",
        "    )\n",
        "\n",
        "    batch_id = batch.id\n",
        "    print(f\"Batch submitted successfully. Batch ID: {batch_id}\")\n",
        "    return batch_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZAh4kE1JShK"
      },
      "outputs": [],
      "source": [
        "def check_batch_status(batch_id, delay=120):\n",
        "    while True:\n",
        "        batch_status = client.batches.retrieve(batch_id)\n",
        "        status = batch_status.status\n",
        "        print(f\"Batch ID: {batch_id} Status: {status}\")\n",
        "\n",
        "        if status == 'completed':\n",
        "            output_file_id = batch_status.output_file_id\n",
        "            return {'status': status, 'output_file_id': output_file_id}\n",
        "        elif status == 'failed':\n",
        "            return {'status': status, 'output_file_id': None}\n",
        "\n",
        "        print(f\"Waiting for {delay} seconds before checking again...\")\n",
        "        time.sleep(delay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mff-O9nWJSjp"
      },
      "outputs": [],
      "source": [
        "def process_and_save_batch(output_file_id, batch_number, save_directory):\n",
        "    file_response = client.files.content(output_file_id)\n",
        "    response_text = file_response.text\n",
        "\n",
        "    batch_df = process_response_to_dataframe(response_text)\n",
        "\n",
        "    batch_df.to_csv(f'{save_directory}/batch_{batch_number}.csv', index=False)\n",
        "    print(f\"Batch {batch_number} saved as batch_{batch_number}.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8DNRlK1JSo0",
        "outputId": "b7b4de6d-c364-4662-8717-386cc7f97cb9"
      },
      "outputs": [],
      "source": [
        "def process_batches(batch_files, save_directory):\n",
        "    for batch_number, (file_path, description) in enumerate(batch_files, start=1):\n",
        "        batch_id = submit_batch(file_path, description)\n",
        "\n",
        "        status_info = check_batch_status(batch_id)\n",
        "\n",
        "        if status_info['status'] == 'completed':\n",
        "            process_and_save_batch(status_info['output_file_id'], batch_number, save_directory)\n",
        "        else:\n",
        "            print(f\"Batch {batch_number} failed or is still in progress. Skipping file processing.\")\n",
        "\n",
        "# Please use batch numbers based on your number of batch files. if the batch is failes, please change input variable start to the batch number for conintuation\n",
        "batch_files = [\n",
        "    (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_11.jsonl\", \"batch_11 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_12.jsonl\", \"batch_12 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_13.jsonl\", \"batch_13 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_14.jsonl\", \"batch_14 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_15.jsonl\", \"batch_15 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_16.jsonl\", \"batch_16 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_17.jsonl\", \"batch_17 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_18.jsonl\", \"batch_18 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_19.jsonl\", \"batch_19 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_20.jsonl\", \"batch_20 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_21.jsonl\", \"batch_21 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_22.jsonl\", \"batch_22 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_23.jsonl\", \"batch_23 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_24.jsonl\", \"batch_24 summarization\"),\n",
        "    # (\"/content/drive/MyDrive/Perspectivism/GPT/batchinput_25.jsonl\", \"batch_25 summarization\"),\n",
        "\n",
        "]\n",
        "\n",
        "save_directory = '/content/drive/MyDrive/Perspectivism/GPT'\n",
        "process_batches(batch_files, save_directory)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
