{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_soft_human.csv')\n",
    "test = pd.read_csv('test_soft_human.csv')\n",
    "val = pd.read_csv('val_soft_human.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(619, 23)\n",
      "(139, 23)\n",
      "(139, 23)\n"
     ]
    }
   ],
   "source": [
    "def no_maj(df):\n",
    "    df = df.loc[df['majority_label'] != 'No majority']\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "train = no_maj(train)\n",
    "test = no_maj(test)\n",
    "val = no_maj(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {'Pro': 0,\n",
    "'Against': 1,\n",
    "'Neutral': 2,\n",
    "'Not-about': 3}\n",
    "\n",
    "train['labels'] = train['majority_label'].map(label_encoding)\n",
    "val['labels'] = val['majority_label'].map(label_encoding)\n",
    "test['labels'] = test['majority_label'].map(label_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['Input', 'labels']]\n",
    "val = val[['Input', 'labels']]\n",
    "test = test[['Input', 'labels']]\n",
    "\n",
    "\n",
    "train['Input'] = train['Input'].str.lower()\n",
    "val['Input'] = val['Input'].str.lower()\n",
    "test['Input'] = test['Input'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = Dataset.from_pandas(train)\n",
    "test_ = Dataset.from_pandas(test)\n",
    "val_ = Dataset.from_pandas(val)\n",
    "\n",
    "\n",
    "dataset = DatasetDict({'train': train_, 'test': test_, 'val': val_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../output/'\n",
    "model_name = 'google-bert/bert-large-uncased' #google-bert/bert-large-uncased'\n",
    "model_name_filename = model_name.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    tokenized_inputs = tokenizer(examples['Input'], padding = 'max_length', truncation = True, max_length = 512)\n",
    "    tokenized_inputs['label'] = examples['labels']\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6e769122b14e1b8e0ce4399aee54f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d49ac256b1b43c4a6de917675865594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee7281323834445b1c6fcd53d61317e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized = train_.map(tokenize_func, batched = True)\n",
    "val_tokenized = val_.map(tokenize_func, batched = True)\n",
    "test_tokenized = test_.map(tokenize_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Input', 'labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 619\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized.set_format('torch', columns = ['input_ids', 'attention_mask','token_type_ids', 'label'])\n",
    "val_tokenized.set_format('torch', columns = ['input_ids', 'attention_mask','token_type_ids', 'label'])\n",
    "test_tokenized.set_format('torch', columns = ['input_ids', 'attention_mask','token_type_ids', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./output/baseline_{model_name_filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    cross_entropy = -np.sum(np.eye(probs.shape[1])[labels] * np.log(probs + 1e-9)) / len(labels)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'cross_entropy': cross_entropy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=train_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpraveenbushipaka942\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nas/users/p.bushipaka/hlt/coling/wandb/run-20240828_155105-ojghokv4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/praveenbushipaka942/huggingface/runs/ojghokv4' target=\"_blank\">pretty-waterfall-43</a></strong> to <a href='https://wandb.ai/praveenbushipaka942/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/praveenbushipaka942/huggingface' target=\"_blank\">https://wandb.ai/praveenbushipaka942/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/praveenbushipaka942/huggingface/runs/ojghokv4' target=\"_blank\">https://wandb.ai/praveenbushipaka942/huggingface/runs/ojghokv4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 06:01, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Cross Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.546100</td>\n",
       "      <td>1.505450</td>\n",
       "      <td>0.205170</td>\n",
       "      <td>0.069857</td>\n",
       "      <td>1.505567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.423400</td>\n",
       "      <td>1.420465</td>\n",
       "      <td>0.219709</td>\n",
       "      <td>0.097953</td>\n",
       "      <td>1.420589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>1.326948</td>\n",
       "      <td>0.369952</td>\n",
       "      <td>0.320858</td>\n",
       "      <td>1.327037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.301000</td>\n",
       "      <td>1.208233</td>\n",
       "      <td>0.484653</td>\n",
       "      <td>0.452118</td>\n",
       "      <td>1.208058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.170200</td>\n",
       "      <td>1.078676</td>\n",
       "      <td>0.630048</td>\n",
       "      <td>0.625338</td>\n",
       "      <td>1.078991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.124600</td>\n",
       "      <td>1.012385</td>\n",
       "      <td>0.602585</td>\n",
       "      <td>0.594844</td>\n",
       "      <td>1.012251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=1.3560953696568807, metrics={'train_runtime': 370.7936, 'train_samples_per_second': 10.016, 'train_steps_per_second': 0.324, 'total_flos': 3461216472686592.0, 'train_loss': 1.3560953696568807, 'epoch': 6.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.bushipaka/miniconda3/envs/hlt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0786762237548828, 'eval_accuracy': 0.630048465266559, 'eval_f1': 0.6253383531773795, 'eval_cross_entropy': 1.078990625064092, 'eval_runtime': 9.3128, 'eval_samples_per_second': 66.468, 'eval_steps_per_second': 2.148, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = f'{output_dir}/best_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./output/baseline_google-bert-bert-large-uncased/best_model/tokenizer_config.json',\n",
       " './output/baseline_google-bert-bert-large-uncased/best_model/special_tokens_map.json',\n",
       " './output/baseline_google-bert-bert-large-uncased/best_model/vocab.txt',\n",
       " './output/baseline_google-bert-bert-large-uncased/best_model/added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(best_model_dir)\n",
    "tokenizer.save_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2533ce3b4f9409da72cbb0297824755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd3c539ed674431864db93d57cb0431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Multiperspective/bert-human-label/commit/7588548b53bb51127cb00fc4ee41cea04cc04f5d', commit_message='Upload tokenizer', commit_description='', oid='7588548b53bb51127cb00fc4ee41cea04cc04f5d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('Multiperspective/bert-human-label')\n",
    "tokenizer.push_to_hub('Multiperspective/bert-human-label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained(best_model_dir)\n",
    "#model = BertForSequenceClassification.from_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation = True, padding = 'max_length', max_length = 512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1).tolist()[0]\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        return probabilities, predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_prob = []\n",
    "softmax_pred = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    text = row['Input']\n",
    "    probs, preds = predictions(text)\n",
    "    softmax_prob.append(probs)\n",
    "    softmax_pred.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['softmax_prob'] = softmax_prob\n",
    "test['softmax_preds'] = softmax_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.69064748201439\n",
      "Precision: 39.035364145658264\n",
      "Recall: 35.93791766907245\n",
      "F1 Score: 33.80628161542504\n",
      "Confusion Matrix:\n",
      "[[ 6  4 26  7]\n",
      " [ 2  6 19  2]\n",
      " [ 5  4 29  5]\n",
      " [ 3  0 11 10]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.14      0.20        43\n",
      "           1       0.43      0.21      0.28        29\n",
      "           2       0.34      0.67      0.45        43\n",
      "           3       0.42      0.42      0.42        24\n",
      "\n",
      "    accuracy                           0.37       139\n",
      "   macro avg       0.39      0.36      0.34       139\n",
      "weighted avg       0.38      0.37      0.33       139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = test['labels']\n",
    "y_pred = test['softmax_preds']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision*100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall*100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1*100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temperature_scaling_bert import TemperatureScalingCalibrationModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 619\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['Input', 'labels', '__index_level_0__'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Input', '__index_level_0__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e1fc7858384446824066209091b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9f30ba24f24eba9e4c0937b729c905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb796b6921924c848ff2d088599d3fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "    # Tokenize the input text\n",
    "    tokenized_example = tokenizer(example['Input'], padding='max_length', truncation=True)\n",
    "    # Add the numerical majority label\n",
    "    tokenized_example['label'] = example['labels']\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_dict = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched= True,\n",
    "    remove_columns = columns\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:24<00:00,  4.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TemperatureScalingCalibrationModule(\n",
       "  (model): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 1024)\n",
       "        (token_type_embeddings): Embedding(2, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_module = TemperatureScalingCalibrationModule(best_model_dir, tokenizer).to(device)\n",
    "calibration_module.fit(tokenized_dict['val'], n_epochs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.9974], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_module.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, examples, round_digits: int = 5):\n",
    "    input_ids = examples['input_ids'].to(device)\n",
    "    attention_mask = examples['attention_mask'].to(device)\n",
    "    token_type_ids = examples['token_type_ids'].to(device)\n",
    "    batch_labels = examples['labels'].detach().cpu().numpy().tolist()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_output = model(input_ids, attention_mask, token_type_ids) #,token_type_ids\n",
    "\n",
    "    batch_scores = np.round(batch_output.detach().cpu().numpy(), round_digits).tolist()\n",
    "    predicted_labels = [np.argmax(scores) for scores in batch_scores]\n",
    "    return batch_scores, batch_labels, predicted_labels\n",
    "\n",
    "\n",
    "def predict_data_loader(model, data_loader: DataLoader) -> pd.DataFrame:\n",
    "    scores = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for examples in data_loader:\n",
    "        batch_scores, batch_labels, batch_pred_labels = predict(model, examples)\n",
    "        scores += batch_scores\n",
    "        true_labels += batch_labels\n",
    "        pred_labels += batch_pred_labels\n",
    "\n",
    "    df_predictions = pd.DataFrame({'scores': scores, 'original_labels': true_labels, 'pred_labels': pred_labels})\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed:  4.228758811950684\n",
      "(139, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>original_labels</th>\n",
       "      <th>pred_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.17215999960899353, 0.3062399923801422, 0.46...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.32607999444007874, 0.13131000101566315, 0.2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2809799909591675, 0.11432000249624252, 0.28...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.251010000705719, 0.1273300051689148, 0.3838...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.2943600118160248, 0.12541000545024872, 0.27...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              scores  original_labels  \\\n",
       "0  [0.17215999960899353, 0.3062399923801422, 0.46...                0   \n",
       "1  [0.32607999444007874, 0.13131000101566315, 0.2...                0   \n",
       "2  [0.2809799909591675, 0.11432000249624252, 0.28...                1   \n",
       "3  [0.251010000705719, 0.1273300051689148, 0.3838...                3   \n",
       "4  [0.2943600118160248, 0.12541000545024872, 0.27...                3   \n",
       "\n",
       "   pred_labels  \n",
       "0            2  \n",
       "1            0  \n",
       "2            3  \n",
       "3            2  \n",
       "4            3  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "data_loader = DataLoader(tokenized_dict['test'], collate_fn=data_collator, batch_size=128)\n",
    "start = time.time()\n",
    "df_calibrated_predictions = predict_data_loader(calibration_module, data_loader)\n",
    "end = time.time()\n",
    "\n",
    "print('elapsed: ', end - start)\n",
    "print(df_calibrated_predictions.shape)\n",
    "df_calibrated_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.69064748201439\n",
      "Precision: 39.035364145658264\n",
      "Recall: 35.93791766907245\n",
      "F1 Score: 33.80628161542504\n",
      "Confusion Matrix:\n",
      "[[ 6  4 26  7]\n",
      " [ 2  6 19  2]\n",
      " [ 5  4 29  5]\n",
      " [ 3  0 11 10]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.14      0.20        43\n",
      "           1       0.43      0.21      0.28        29\n",
      "           2       0.34      0.67      0.45        43\n",
      "           3       0.42      0.42      0.42        24\n",
      "\n",
      "    accuracy                           0.37       139\n",
      "   macro avg       0.39      0.36      0.34       139\n",
      "weighted avg       0.38      0.37      0.33       139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = df_calibrated_predictions['original_labels']\n",
    "y_pred = df_calibrated_predictions['pred_labels']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision*100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall*100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1*100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>labels</th>\n",
       "      <th>softmax_prob</th>\n",
       "      <th>softmax_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>should social security be privatized? social s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.17241421341896057, 0.30623528361320496, 0.4...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can alternative energy effectively replace fos...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.32589882612228394, 0.1315443515777588, 0.27...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>should the united states maintain its embargo ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.28093746304512024, 0.11456725001335144, 0.2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>should the united states return to a gold stan...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.2510535717010498, 0.12757351994514465, 0.38...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is obesity a disease? treatment for obesity an...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.29427507519721985, 0.12564603984355927, 0.2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input  labels  \\\n",
       "0  should social security be privatized? social s...       0   \n",
       "1  can alternative energy effectively replace fos...       0   \n",
       "2  should the united states maintain its embargo ...       1   \n",
       "3  should the united states return to a gold stan...       3   \n",
       "4  is obesity a disease? treatment for obesity an...       3   \n",
       "\n",
       "                                        softmax_prob  softmax_preds  \n",
       "0  [0.17241421341896057, 0.30623528361320496, 0.4...              2  \n",
       "1  [0.32589882612228394, 0.1315443515777588, 0.27...              0  \n",
       "2  [0.28093746304512024, 0.11456725001335144, 0.2...              3  \n",
       "3  [0.2510535717010498, 0.12757351994514465, 0.38...              2  \n",
       "4  [0.29427507519721985, 0.12564603984355927, 0.2...              3  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>original_labels</th>\n",
       "      <th>pred_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.17215999960899353, 0.3062399923801422, 0.46...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.32607999444007874, 0.13131000101566315, 0.2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2809799909591675, 0.11432000249624252, 0.28...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.251010000705719, 0.1273300051689148, 0.3838...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.2943600118160248, 0.12541000545024872, 0.27...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              scores  original_labels  \\\n",
       "0  [0.17215999960899353, 0.3062399923801422, 0.46...                0   \n",
       "1  [0.32607999444007874, 0.13131000101566315, 0.2...                0   \n",
       "2  [0.2809799909591675, 0.11432000249624252, 0.28...                1   \n",
       "3  [0.251010000705719, 0.1273300051689148, 0.3838...                3   \n",
       "4  [0.2943600118160248, 0.12541000545024872, 0.27...                3   \n",
       "\n",
       "   pred_labels  \n",
       "0            2  \n",
       "1            0  \n",
       "2            3  \n",
       "3            2  \n",
       "4            3  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calibrated_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_soft_human.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['uncalib_scores'] = softmax_prob\n",
    "test_df['uncalib_preds'] = softmax_pred\n",
    "test_df['calib_scores'] = df_calibrated_predictions['scores'].tolist()\n",
    "test_df['calib_preds'] = df_calibrated_predictions['pred_labels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('results_baseline_bert_human.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
