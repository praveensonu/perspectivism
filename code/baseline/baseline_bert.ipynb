{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_path = 'please use the dataset path here - Human dataset'\n",
    "\n",
    "\n",
    "train = pd.read_csv(NDtrainpath)\n",
    "val = pd.read_csv(HDvalpath)\n",
    "test = pd.read_csv(HDtestpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_maj(df):\n",
    "    df = df.loc[df['majority_label'] != 'No majority']\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "train = no_maj(train)\n",
    "test = no_maj(test)\n",
    "val = no_maj(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {'Pro': 0,\n",
    "'Against': 1,\n",
    "'Neutral': 2,\n",
    "'Not-about': 3}\n",
    "\n",
    "train['labels'] = train['majority_label'].map(label_encoding)\n",
    "val['labels'] = val['majority_label'].map(label_encoding)\n",
    "test['labels'] = test['majority_label'].map(label_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['Input', 'labels']]\n",
    "val = val[['Input', 'labels']]\n",
    "test = test[['Input', 'labels']]\n",
    "\n",
    "\n",
    "train['Input'] = train['Input'].str.lower()\n",
    "val['Input'] = val['Input'].str.lower()\n",
    "test['Input'] = test['Input'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = Dataset.from_pandas(train)\n",
    "test_ = Dataset.from_pandas(test)\n",
    "val_ = Dataset.from_pandas(val)\n",
    "\n",
    "\n",
    "dataset = DatasetDict({'train': train_, 'test': test_, 'val': val_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../output/'\n",
    "model_name = 'google-bert/bert-large-uncased' #google-bert/bert-large-uncased'\n",
    "model_name_filename = model_name.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    tokenized_inputs = tokenizer(examples['Input'], padding = 'max_length', truncation = True, max_length = 512)\n",
    "    tokenized_inputs['label'] = examples['labels']\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_.map(tokenize_func, batched = True)\n",
    "val_tokenized = val_.map(tokenize_func, batched = True)\n",
    "test_tokenized = test_.map(tokenize_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized.set_format('torch', columns = ['input_ids', 'attention_mask','token_type_ids', 'label'])\n",
    "val_tokenized.set_format('torch', columns = ['input_ids', 'attention_mask','token_type_ids', 'label'])\n",
    "test_tokenized.set_format('torch', columns = ['input_ids', 'attention_mask','token_type_ids', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./output/baseline_{model_name_filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    cross_entropy = -np.sum(np.eye(probs.shape[1])[labels] * np.log(probs + 1e-9)) / len(labels)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'cross_entropy': cross_entropy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=train_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = f'{output_dir}/best_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(best_model_dir)\n",
    "tokenizer.save_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation = True, padding = 'max_length', max_length = 512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1).tolist()[0]\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        return probabilities, predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_prob = []\n",
    "softmax_pred = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    text = row['Input']\n",
    "    probs, preds = predictions(text)\n",
    "    softmax_prob.append(probs)\n",
    "    softmax_pred.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['softmax_prob'] = softmax_prob\n",
    "test['softmax_preds'] = softmax_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['labels']\n",
    "y_pred = test['softmax_preds']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision*100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall*100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1*100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temperature_scaling_bert import TemperatureScalingCalibrationModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Input', '__index_level_0__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    # Tokenize the input text\n",
    "    tokenized_example = tokenizer(example['Input'], padding='max_length', truncation=True)\n",
    "    # Add the numerical majority label\n",
    "    tokenized_example['label'] = example['labels']\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_dict = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched= True,\n",
    "    remove_columns = columns\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_module = TemperatureScalingCalibrationModule(best_model_dir, tokenizer).to(device)\n",
    "calibration_module.fit(tokenized_dict['val'], n_epochs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_module.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, examples, round_digits: int = 5):\n",
    "    input_ids = examples['input_ids'].to(device)\n",
    "    attention_mask = examples['attention_mask'].to(device)\n",
    "    token_type_ids = examples['token_type_ids'].to(device)\n",
    "    batch_labels = examples['labels'].detach().cpu().numpy().tolist()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_output = model(input_ids, attention_mask, token_type_ids) #,token_type_ids\n",
    "\n",
    "    batch_scores = np.round(batch_output.detach().cpu().numpy(), round_digits).tolist()\n",
    "    predicted_labels = [np.argmax(scores) for scores in batch_scores]\n",
    "    return batch_scores, batch_labels, predicted_labels\n",
    "\n",
    "\n",
    "def predict_data_loader(model, data_loader: DataLoader) -> pd.DataFrame:\n",
    "    scores = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for examples in data_loader:\n",
    "        batch_scores, batch_labels, batch_pred_labels = predict(model, examples)\n",
    "        scores += batch_scores\n",
    "        true_labels += batch_labels\n",
    "        pred_labels += batch_pred_labels\n",
    "\n",
    "    df_predictions = pd.DataFrame({'scores': scores, 'original_labels': true_labels, 'pred_labels': pred_labels})\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "data_loader = DataLoader(tokenized_dict['test'], collate_fn=data_collator, batch_size=128)\n",
    "start = time.time()\n",
    "df_calibrated_predictions = predict_data_loader(calibration_module, data_loader)\n",
    "end = time.time()\n",
    "\n",
    "print('elapsed: ', end - start)\n",
    "print(df_calibrated_predictions.shape)\n",
    "df_calibrated_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_calibrated_predictions['original_labels']\n",
    "y_pred = df_calibrated_predictions['pred_labels']\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision*100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall*100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1*100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calibrated_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_soft_human.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['uncalib_scores'] = softmax_prob\n",
    "test_df['uncalib_preds'] = softmax_pred\n",
    "test_df['calib_scores'] = df_calibrated_predictions['scores'].tolist()\n",
    "test_df['calib_preds'] = df_calibrated_predictions['pred_labels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('results_baseline_bert_human.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
